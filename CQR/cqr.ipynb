{"cells":[{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14850,"status":"ok","timestamp":1682670935683,"user":{"displayName":"Prénom Nom","userId":"00284320685744777734"},"user_tz":-120},"id":"b8a78120-c998-4511-a92c-3abf1dd97757","outputId":"892acfb3-37db-40be-d139-ea09b460d336"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.98)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.28.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (2023.4.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.15.0)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0)\n","Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.27.1)\n","Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.31)\n","Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.21.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n","Requirement already satisfied: pathtools in /usr/local/lib/python3.10/dist-packages (from wandb) (0.1.2)\n","Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.3)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n","Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n","Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\n","Requirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.2)\n","Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n","Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.12)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n","Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","/content/gdrive/MyDrive/CQR\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import os\n","import re\n","import sys\n","import torch\n","\n","!pip install sentencepiece\n","!pip install transformers\n","!pip install wandb\n","\n","from transformers import T5Tokenizer, T5ForConditionalGeneration\n","import wandb\n","\n","from torch.utils.data import Dataset, DataLoader\n","from torch.utils.data.dataloader import default_collate\n","from torch.nn.utils.rnn import pad_sequence\n","from torch import cuda\n","\n","\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","%cd /content/gdrive/MyDrive/CQR/\n","\n","path = \".\""],"id":"b8a78120-c998-4511-a92c-3abf1dd97757"},{"cell_type":"markdown","metadata":{"id":"J62k_5W0O62t"},"source":["Extraction des données de CANARD sous forme de DataFrame (tableau à deux colonnes) pour les donner au modèle.\n","\n","*   col1 = CONTEXT = Contexte ( Requête non reformulée + requêtes précédentes et réponses associées)\n","*   col 2 = REWRITE = Requête reformulée à obtenir\n","\n"],"id":"J62k_5W0O62t"},{"cell_type":"code","execution_count":21,"metadata":{"id":"p-WJs0eESUwG","executionInfo":{"status":"ok","timestamp":1682670935684,"user_tz":-120,"elapsed":10,"user":{"displayName":"Prénom Nom","userId":"00284320685744777734"}}},"outputs":[],"source":["def read_trec(year):\n","\tassert year==2020 or year==2021\n","\ttest = pd.read_csv(f'{path}/TREC/trec{year}.csv').reset_index()\n","\treturn test \n","\n","def collectData(): \n","  #données entrainement\n","  train =  pd.read_csv(f'{path}/CANARD/train_.csv')\n","  #données test\n","  test = pd.read_csv(f'{path}/CANARD/test_.csv')\n","\n","  return train[[\"Context\",\"Rewrite\"]], test[[\"Context\",\"Rewrite\"]]"],"id":"p-WJs0eESUwG"},{"cell_type":"markdown","metadata":{"id":"bdFRXOvlPosn"},"source":["\n","Classe qui utilise les DataFrame (tableau à deux colonnes) collectées précédemment pour les donner au modèle\n"],"id":"bdFRXOvlPosn"},{"cell_type":"code","execution_count":22,"metadata":{"id":"HmiA6xhyRSGU","executionInfo":{"status":"ok","timestamp":1682670935684,"user_tz":-120,"elapsed":8,"user":{"displayName":"Prénom Nom","userId":"00284320685744777734"}}},"outputs":[],"source":["#Classe pour lire et rendre compréhensible un dataframe et le passer plus tard au modèle\n","class Dataset_2(Dataset):\n","\tdef __init__(self, dataframe, tokenizer, context_len = 512, rewrite_len = 128):\n","\t\tself.tokenizer = tokenizer\n","\t\tself.rewrite = dataframe.Rewrite # Questions à obtenir après reformulation\n","\t\tself.context = dataframe.Context # Contextes\n","\n","\t\tself.context_len = context_len #Longueur max du contexte pour l'encodage\n","\t\tself.rewrite_len = rewrite_len #Longueur max de la question réecrite pour l'encodage\n","\n","\tdef __len__(self):\n","\t\treturn len(self.context) #Taille du dataset\n","\n","\t#Retourne masque d'attention et input ids de la ligne index du dataframe\n","\tdef __getitem__(self, index):\n","\t\t#Dataframe sous forme de liste puis encoder avec le tokenizer (masque d'attention et input_ids)\n","\t\tcontext = str(self.context[index])\n","\t\tcontext = ' '.join(context.split())\n","\t\tcontext = self.tokenizer.batch_encode_plus([context], max_length= self.context_len, padding='longest',return_tensors='pt',truncation=True)\n","\t\tcontext_ids = context['input_ids'].squeeze()\n","\t\tcontext_mask = context['attention_mask'].squeeze()\n","\t\n","\t\trewrite = str(self.rewrite[index])\n","\t\trewrite = ' '.join(rewrite.split())\n","\t\trewrite = self.tokenizer.batch_encode_plus([rewrite], max_length= self.rewrite_len, padding='longest',return_tensors='pt',truncation=True)\n","\t\trewrite_ids = rewrite['input_ids'].squeeze()\n","\t\trewrite_mask = rewrite['attention_mask'].squeeze()\n","\n","\t\treturn {\n","\t\t\t'context_ids': context_ids.to(dtype=torch.long), \n","\t\t\t'context_mask': context_mask.to(dtype=torch.long), \n","\t\t\t'rewrite_ids': rewrite_ids.to(dtype=torch.long),\n","\t\t\t'rewrite_mask': rewrite_mask.to(dtype=torch.long),\n","\t\t}"],"id":"HmiA6xhyRSGU"},{"cell_type":"code","execution_count":23,"metadata":{"id":"Oi5ooTjtaj3f","executionInfo":{"status":"ok","timestamp":1682670935685,"user_tz":-120,"elapsed":9,"user":{"displayName":"Prénom Nom","userId":"00284320685744777734"}}},"outputs":[],"source":["# Recupère les masques d'attentions/input_ids des contextes/reformulation du batch\n","def my_collate(batch):\n","\tpadded_context_ids = pad_sequence([item['context_ids'] for item in batch], batch_first=True)\n","\tpadded_context_mask = pad_sequence([item['context_mask'] for item in batch], batch_first=True)\n","\tpadded_rewrite_ids = pad_sequence([item['rewrite_ids'] for item in batch], batch_first=True)\n","\tpadded_rewrite_mask = pad_sequence([item['rewrite_mask'] for item in batch], batch_first=True)\t\n","\t\n","\tbatch = [{'context_ids':padded_context_ids[i], 'context_mask':padded_context_mask[i],\\\n","\t\t\t      'rewrite_ids':padded_rewrite_ids[i]} for i in range(len(padded_context_ids))]\n","\treturn default_collate(batch)"],"id":"Oi5ooTjtaj3f"},{"cell_type":"code","execution_count":24,"metadata":{"id":"hVvmI1IWRbzs","executionInfo":{"status":"ok","timestamp":1682670935685,"user_tz":-120,"elapsed":8,"user":{"displayName":"Prénom Nom","userId":"00284320685744777734"}}},"outputs":[],"source":["# fonction d'entrainement du modèle.\n","# Inspirer de Nam Le Hai\n","def train(tokenizer, model, device, loader, optimizer):\n","\tmodel.train()\n","\ti=1\n","\tfor _,data in enumerate(loader, 0):\n","\t\ty = data['rewrite_ids'].to(device, dtype = torch.long)\n","\t\ty_ids = y[:, :-1].contiguous()\n","\t\tlabels = y[:, 1:].clone().detach()\n","\t\tlabels[y[:, 1:] == tokenizer.pad_token_id] = -100\n","\t\tids = data['context_ids'].to(device, dtype = torch.long)\n","\t\tmask = data['context_mask'].to(device, dtype = torch.long)\n","\n","\t\toutputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, labels=labels)\n","\t\tloss = outputs[0]\n","\t\t\n","\t\toptimizer.zero_grad()\n","\t\tloss.backward()\n","\t\toptimizer.step()\n","\t\ti+=1"],"id":"hVvmI1IWRbzs"},{"cell_type":"code","execution_count":25,"metadata":{"id":"_taBDElyReqA","executionInfo":{"status":"ok","timestamp":1682670935686,"user_tz":-120,"elapsed":9,"user":{"displayName":"Prénom Nom","userId":"00284320685744777734"}}},"outputs":[],"source":["def reformulate(tokenizer, model, device, loader):\n","\tmodel.eval()\n","\tpredictions = []\n","\tactuals = []\n","\twith torch.no_grad():\n","\t\tfor _, data in enumerate(loader, 0):\n","\t\t\ty = data['rewrite_ids'].to(device, dtype = torch.long)\n","\t\t\tids = data['context_ids'].to(device, dtype = torch.long)\n","\t\t\tmask = data['context_mask'].to(device, dtype = torch.long)\n","\n","\t\t\tgenerated_ids = model.generate(\n","\t\t\t\tinput_ids = ids,\n","\t\t\t\tattention_mask = mask, \n","\t\t\t\tmax_length=128, \n","\t\t\t\tnum_beams=3,\n","\t\t\t\trepetition_penalty=2.5, \n","\t\t\t\tlength_penalty=1.0, \n","\t\t\t\tearly_stopping=True\n","\t\t\t\t)\n","\t\t\tpreds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n","\t\t\trewrite = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n","\t\t\n","\t\t\tpredictions.extend(preds)\n","\t\t\tactuals.extend(rewrite)\n","\treturn predictions, actuals\n"],"id":"_taBDElyReqA"},{"cell_type":"code","execution_count":26,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"id":"bh6q2EAERiXk","outputId":"4793f90e-5e1f-4bc3-d147-70c55302b419","executionInfo":{"status":"error","timestamp":1682670946051,"user_tz":-120,"elapsed":10373,"user":{"displayName":"Prénom Nom","userId":"00284320685744777734"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Entraînement époque 0\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-26-70fe0ae1395f>\u001b[0m in \u001b[0;36m<cell line: 37>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Entraînement époque {epoch}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Fin entraînement époque {epoch}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-24-0e0a2c5d068f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(tokenizer, model, device, loader, optimizer)\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                 \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'rewrite_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m                 \u001b[0my_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                 \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["device = 'cuda'\n","\n","torch.cuda.empty_cache()\n","\n","#wandb.init(project=\"pldac\")\n","#wandb.run.name = \"CQR\"\n","\n","# Hyperparametres du modèle, wandb les garde en mémoire\n","#config = wandb.config        \n","\n","# pytorch random seed\n","torch.manual_seed(0)\n","\n","# T5 pour l'encodage du texte\n","tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n","\n","# dataframe de train et de test\n","train_df, test_df = collectData()\n","\n","# dataset de train et de test\n","training_ds = Dataset_2(train_df, tokenizer)\n","test_ds = Dataset_2(test_df, tokenizer)\n","\n","#DataLoaders d'entrainement et de test.\n","training_dl = DataLoader(training_ds, batch_size = 8, shuffle = True, collate_fn = my_collate)\n","test_dl = DataLoader(test_ds, batch_size = 8, collate_fn = my_collate)\n","\n","# Modèle T5-base  \n","model = T5ForConditionalGeneration.from_pretrained(\"t5-base\",cache_dir=\"/tmp\").to(device)\n","\n","# Optimizer qui va modifier les poids du réseau pendant l'apprentissage. learning rate de 1e-3 \n","optimizer = torch.optim.Adam(params =  model.parameters()) #, lr = 1e-3 par défaut\n","\n","# Log metrics with wandb\n","# wandb.watch(model, log=\"all\")\n","\n","for epoch in range(4): \n","\t# Training \n","\tprint(f'Entraînement époque {epoch}')\n","\ttrain(tokenizer, model, device, training_dl, optimizer)\n","\tprint(f'Fin entraînement époque {epoch}')\n","\n","\t# Validation et sauvegarde des resultats sous forme de DataFrame\n","\tprint(f'Génération et sauvegarde des reformulations pour epoch {epoch}' )\n","\tprediction, rewrited = reformulate(tokenizer, model, device, test_dl)\n","\treformulation_df = pd.DataFrame({'Prediction':prediction,'Rewrite':rewrited})\n","\toutdir = f'{path}/REFORMULATION/'\t\n","\treformulation_df.to_csv(f'{outdir}/reformulation_{epoch}.csv',sep='\\t')\n","\tprint(f'Reformulations pour époque {epoch} générées')\n","\n","print(\"finito\")"],"id":"bh6q2EAERiXk"},{"cell_type":"code","source":[],"metadata":{"id":"Xw7VTbj_8MAK"},"id":"Xw7VTbj_8MAK","execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"},"vscode":{"interpreter":{"hash":"b37f6842cfa5693682ca177646b86562e7e6680399d6aa97975ae5063d95e764"}},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":5}